{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SIR model: inverse problem\n",
    "## A PINN approach\n",
    "\n",
    "In this notebook, we will solve the inverse problem of the SIR model using a Physics-Informed Neural Network (PINN). The goal is to estimate the infection rate $\\beta$ from the observed data of the infected population. To do this, we will train a PINN model, where we compute the residuals of the differential equation system with initial conditions and the data loss simultaneously.\n",
    "\n",
    "The SIR model is governed by the following set of ordinary differential equations (ODEs):\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{dS}{dt} &= -\\frac{\\beta}{N} I S, \\\\\n",
    "\\frac{dI}{dt} &= \\frac{\\beta}{N} I S - \\delta I, \\\\\n",
    "\\frac{dR}{dt} &= \\delta I,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $t \\in [0, 90]$ and with the initial conditions $S(0) = N - 1$, $I(0) = 1$, and $R(0) = 0$."
   ],
   "id": "65fffccded80afc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "e2b13f713fe59120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create directories\n",
    "figures_dir = \"figures\"\n",
    "data_dir = \"data\"\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Configuration dictionary\n",
    "config = {\n",
    "  # Data parameters\n",
    "  \"t_start\":            0,\n",
    "  \"t_end\":              90,\n",
    "\n",
    "  # Model parameters\n",
    "  \"population\":         1.0,  # Normalized population\n",
    "  \"delta\":              1 / 5,  # Recovery rate\n",
    "  \"initial_beta\":       0.5,  # Initial value for beta parameter\n",
    "\n",
    "  # Network architecture\n",
    "  \"layers\":             [1] + [50] * 4 + [1],\n",
    "  \"activation\":         \"tanh\",\n",
    "  \"output_activation\":  \"square\",\n",
    "\n",
    "  # Training parameters\n",
    "  \"learning_rate\":      1e-3,\n",
    "  \"batch_size\":         100,\n",
    "  \"dataset_size\":       6000,\n",
    "  \"max_epochs\":         5000,\n",
    "\n",
    "  # Early stopping\n",
    "  \"patience\":           200,\n",
    "  \"min_delta\":          1e-5,\n",
    "\n",
    "  # Loss weights\n",
    "  \"weight_pde\":         1.0,\n",
    "  \"weight_ic\":          1.0,\n",
    "  \"weight_data\":        1.0,\n",
    "\n",
    "  # Optimizer settings\n",
    "  \"clip_grad_norm\":     1.0,\n",
    "\n",
    "  # Scheduler\n",
    "  \"scheduler_patience\": 100,\n",
    "  \"scheduler_factor\":   0.5,\n",
    "  \"min_lr\":             1e-6,\n",
    "\n",
    "  # Logging\n",
    "  \"log_interval\":       100,\n",
    "}\n"
   ],
   "id": "4e31b153caf564bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utility functions",
   "id": "7120ac1ce626e51c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Square(nn.Module):\n",
    "  \"\"\"A module that squares its input element-wise.\"\"\"\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(x):\n",
    "    return torch.square(x)\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "  \"\"\"Get activation function by name.\"\"\"\n",
    "  if name == \"tanh\":\n",
    "    return nn.Tanh()\n",
    "  elif name == \"relu\":\n",
    "    return nn.ReLU()\n",
    "  elif name == \"sigmoid\":\n",
    "    return nn.Sigmoid()\n",
    "  elif name == \"square\":\n",
    "    return Square()\n",
    "  elif name == \"softplus\":\n",
    "    return nn.Softplus()\n",
    "  elif name == \"none\":\n",
    "    return nn.Identity()\n",
    "  else:\n",
    "    raise ValueError(f\"Activation function {name} not recognized\")\n",
    "\n",
    "\n",
    "def create_fnn(layers_dimensions, activation, output_activation):\n",
    "  \"\"\"Create a feedforward neural network with specified architecture.\"\"\"\n",
    "  layers_modules = []\n",
    "  for i in range(len(layers_dimensions) - 1):\n",
    "    layers_modules.append(\n",
    "      nn.Linear(layers_dimensions[i], layers_dimensions[i + 1])\n",
    "    )\n",
    "    if i < len(layers_dimensions) - 2:\n",
    "      layers_modules.append(activation)\n",
    "\n",
    "  if output_activation is not None:\n",
    "    layers_modules.append(output_activation)\n",
    "\n",
    "  net = nn.Sequential(*layers_modules)\n",
    "\n",
    "  # Initialize weights and biases\n",
    "  for m in net:\n",
    "    if isinstance(m, nn.Linear):\n",
    "      nn.init.xavier_normal_(m.weight)\n",
    "      nn.init.zeros_(m.bias)\n",
    "\n",
    "  return net\n"
   ],
   "id": "7602c2b769e44162"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model definition",
   "id": "cb36a9738f8dd15c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SIRPINN(pl.LightningModule):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters(config)\n",
    "    self.config = config\n",
    "\n",
    "    # Get activation functions\n",
    "    activation = get_activation(config[\"activation\"])\n",
    "    output_activation = get_activation(config[\"output_activation\"])\n",
    "\n",
    "    # Create neural networks\n",
    "    self.net_S = create_fnn(config[\"layers\"], activation, output_activation)\n",
    "    self.net_I = create_fnn(config[\"layers\"], activation, output_activation)\n",
    "\n",
    "    # Model parameters\n",
    "    self.beta = nn.Parameter(\n",
    "      torch.tensor(config[\"initial_beta\"], dtype=torch.float32)\n",
    "    )\n",
    "    self.delta = config[\"delta\"]\n",
    "    self.N = config[\"population\"]\n",
    "\n",
    "    # Loss tracking\n",
    "    self.l_pde_history = []\n",
    "    self.l_ic_history = []\n",
    "    self.l_data_history = []\n",
    "    self.l_total_history = []\n",
    "    self.beta_evolution = []\n",
    "    self.param_changes = []\n",
    "    self.re_s_history = []\n",
    "    self.re_i_history = []\n",
    "\n",
    "    # Store previous parameters for tracking changes\n",
    "    self.old_params = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Forward pass to compute S, I, R values at time points x.\"\"\"\n",
    "    S = self.net_S(x)\n",
    "    I = self.net_I(x)\n",
    "    R = torch.ones_like(S) * self.N - S - I\n",
    "    return torch.cat([S, I, R], dim=1)\n",
    "\n",
    "  def compute_pde_residuals(self, x):\n",
    "    \"\"\"Compute residuals of SIR ODE system.\"\"\"\n",
    "    x.requires_grad_(True)\n",
    "    S = self.net_S(x)\n",
    "    I = self.net_I(x)\n",
    "\n",
    "    dS_dt = torch.autograd.grad(\n",
    "      S, x, grad_outputs=torch.ones_like(S), create_graph=True\n",
    "    )[0]\n",
    "    dI_dt = torch.autograd.grad(\n",
    "      I, x, grad_outputs=torch.ones_like(I), create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    res_S = dS_dt + self.beta * S * I\n",
    "    res_I = dI_dt - self.beta * S * I + self.delta * I\n",
    "    return res_S, res_I\n",
    "\n",
    "  def loss_pde(self, x):\n",
    "    \"\"\"Compute PDE residual loss.\"\"\"\n",
    "    res_S, res_I = self.compute_pde_residuals(x)\n",
    "    return torch.mean(res_S ** 2) + torch.mean(res_I ** 2)\n",
    "\n",
    "  def loss_ic(self, t0_tensor, ic_tensor):\n",
    "    \"\"\"Compute initial condition loss.\"\"\"\n",
    "    ic_pred = self(t0_tensor)\n",
    "    return torch.mean((ic_pred - ic_tensor) ** 2)\n",
    "\n",
    "  def loss_data(self, t_obs_tensor, i_obs_tensor):\n",
    "    \"\"\"Compute data fitting loss.\"\"\"\n",
    "    I = self(t_obs_tensor)[:, 1].reshape(-1, 1)\n",
    "    return torch.mean((I - i_obs_tensor) ** 2)\n",
    "\n",
    "  def compute_param_change(self):\n",
    "    \"\"\"Compute L2 norm of parameter changes between steps.\"\"\"\n",
    "    if self.old_params is None:\n",
    "      self.old_params = [p.clone().detach() for p in self.parameters()]\n",
    "      return 0.0\n",
    "\n",
    "    param_diff_squared_sum = 0.0\n",
    "    new_params = [p.clone().detach() for p in self.parameters()]\n",
    "\n",
    "    for old_p, new_p in zip(self.old_params, new_params):\n",
    "      param_diff_squared_sum += torch.sum((old_p - new_p) ** 2).item()\n",
    "\n",
    "    self.old_params = new_params\n",
    "    return np.sqrt(param_diff_squared_sum)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"\"\"Configure optimizer and learning rate scheduler.\"\"\"\n",
    "    optimizer = torch.optim.Adam(\n",
    "      self.parameters(),\n",
    "      lr=self.config[\"learning_rate\"]\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "      optimizer,\n",
    "      mode='min',\n",
    "      factor=self.config[\"scheduler_factor\"],\n",
    "      patience=self.config[\"scheduler_patience\"],\n",
    "      min_lr=self.config[\"min_lr\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      \"optimizer\":    optimizer,\n",
    "      \"lr_scheduler\": {\n",
    "        \"scheduler\": scheduler,\n",
    "        \"monitor\":   \"val_loss\",\n",
    "        \"interval\":  \"epoch\",\n",
    "        \"frequency\": 1\n",
    "      }\n",
    "    }\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    t_batch, is_obs, t0_tensor, ic_tensor, t_obs_indices = batch\n",
    "\n",
    "    # Compute PDE loss\n",
    "    L_pde = self.loss_pde(t_batch)\n",
    "\n",
    "    # Compute initial condition loss\n",
    "    L_ic = self.loss_ic(t0_tensor, ic_tensor)\n",
    "\n",
    "    # Compute data loss if there are observation points in the batch\n",
    "    L_data = torch.tensor(0.0, device=self.device)\n",
    "    if is_obs.any():\n",
    "      obs_points = t_batch[is_obs]\n",
    "      outputs = self(obs_points)\n",
    "      predicted_I = outputs[:, 1].reshape(-1, 1)\n",
    "\n",
    "      # Get corresponding observation values\n",
    "      obs_indices = t_obs_indices[is_obs]\n",
    "      true_I = self.trainer.datamodule.i_obs[obs_indices].to(self.device)\n",
    "\n",
    "      L_data = torch.mean((predicted_I - true_I) ** 2)\n",
    "\n",
    "    # Compute weighted total loss\n",
    "    loss = (self.config[\"weight_pde\"] * L_pde +\n",
    "            self.config[\"weight_ic\"] * L_ic +\n",
    "            self.config[\"weight_data\"] * L_data)\n",
    "\n",
    "    # Log losses\n",
    "    self.log('train_loss_pde', L_pde, prog_bar=False)\n",
    "    self.log('train_loss_ic', L_ic, prog_bar=False)\n",
    "    self.log('train_loss_data', L_data, prog_bar=False)\n",
    "    self.log('train_loss', loss, prog_bar=True)\n",
    "    self.log('beta', self.beta.item(), prog_bar=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    \"\"\"Validation step.\"\"\"\n",
    "    t_all, t0_tensor, ic_tensor, t_obs_tensor, i_obs_tensor = batch\n",
    "\n",
    "    # Compute PDE loss on all validation points\n",
    "    L_pde = self.loss_pde(t_all)\n",
    "\n",
    "    # Compute initial condition loss\n",
    "    L_ic = self.loss_ic(t0_tensor, ic_tensor)\n",
    "\n",
    "    # Compute data loss on all observation points\n",
    "    L_data = self.loss_data(t_obs_tensor, i_obs_tensor)\n",
    "\n",
    "    # Compute total loss\n",
    "    val_loss = L_pde + L_ic + L_data\n",
    "\n",
    "    # Store losses for history\n",
    "    self.l_pde_history.append(L_pde.item())\n",
    "    self.l_ic_history.append(L_ic.item())\n",
    "    self.l_data_history.append(L_data.item())\n",
    "    self.l_total_history.append(val_loss.item())\n",
    "\n",
    "    # Store beta and compute parameter changes\n",
    "    self.beta_evolution.append(self.beta.item())\n",
    "    param_change = self.compute_param_change()\n",
    "    self.param_changes.append(param_change)\n",
    "\n",
    "    # Compute relative errors if true data is available\n",
    "    if hasattr(self.trainer.datamodule, 'true_data'):\n",
    "      t_np = t_obs_tensor.cpu().detach().numpy().flatten()\n",
    "      sir_pred = self(t_np).cpu().detach().numpy()\n",
    "      s_pred, i_pred, r_pred = sir_pred[:, 0], sir_pred[:, 1], sir_pred[:, 2]\n",
    "\n",
    "      s_true = self.trainer.datamodule.true_data['s_true']\n",
    "      i_true = self.trainer.datamodule.true_data['i_true']\n",
    "\n",
    "      re_s = np.linalg.norm(s_true - s_pred, 2) / np.linalg.norm(s_true, 2)\n",
    "      re_i = np.linalg.norm(i_true - i_pred, 2) / np.linalg.norm(i_true, 2)\n",
    "\n",
    "      self.re_s_history.append(re_s)\n",
    "      self.re_i_history.append(re_i)\n",
    "\n",
    "      self.log('val_re_s', re_s, prog_bar=False)\n",
    "      self.log('val_re_i', re_i, prog_bar=False)\n",
    "\n",
    "    # Log validation metrics\n",
    "    self.log('val_loss_pde', L_pde, prog_bar=False)\n",
    "    self.log('val_loss_ic', L_ic, prog_bar=False)\n",
    "    self.log('val_loss_data', L_data, prog_bar=False)\n",
    "    self.log('val_loss', val_loss, prog_bar=True)\n",
    "    self.log('param_change', param_change, prog_bar=False)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "  def on_validation_epoch_end(self):\n",
    "    \"\"\"Called at the end of validation epoch.\"\"\"\n",
    "    if (self.current_epoch + 1) % self.config[\"log_interval\"] == 0:\n",
    "      print(\n",
    "        f\"[{self.current_epoch + 1:>{len(str(self.config['max_epochs']))}}/{self.config['max_epochs']}]: \"\n",
    "        f\"β = {self.beta.item():.4f} \"\n",
    "        f\"| Loss = {self.l_total_history[-1]:.2e} \"\n",
    "        f\"| Param Δ = {self.param_changes[-1]:.2e} \"\n",
    "        f\"| lr = {self.optimizers().param_groups[0]['lr']:.2e}\"\n",
    "      )\n",
    "\n",
    "  def on_train_end(self):\n",
    "    \"\"\"Save training history when training completes.\"\"\"\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Create DataFrames\n",
    "    loss_df = pd.DataFrame(\n",
    "      {\n",
    "        'L_pde':   self.l_pde_history,\n",
    "        'L_ic':    self.l_ic_history,\n",
    "        'L_data':  self.l_data_history,\n",
    "        'L_total': self.l_total_history\n",
    "      }\n",
    "    )\n",
    "\n",
    "    evolution_df = pd.DataFrame(\n",
    "      {\n",
    "        'beta':   self.beta_evolution,\n",
    "        'params': self.param_changes,\n",
    "        're_s':   self.re_s_history,\n",
    "        're_i':   self.re_i_history\n",
    "      }\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    loss_df.to_csv(\n",
    "      f\"{data_dir}/loss_history_{timestamp}.csv\",\n",
    "      float_format='%.6e'\n",
    "    )\n",
    "    evolution_df.to_csv(\n",
    "      f\"{data_dir}/evolutions_{timestamp}.csv\",\n",
    "      float_format='%.6e'\n",
    "    )\n",
    "\n",
    "    # Save config\n",
    "    with open(f\"{data_dir}/config_{timestamp}.json\", \"w\") as f:\n",
    "      json.dump(\n",
    "        {k: v if not isinstance(v, (np.float32, np.float64)) else float(v)\n",
    "          for k, v in self.config.items()},\n",
    "        f,\n",
    "        indent=2\n",
    "      )\n"
   ],
   "id": "998d6e196fb0334a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
