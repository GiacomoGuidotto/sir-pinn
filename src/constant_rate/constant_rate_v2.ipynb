{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "65fffccded80afc6",
            "metadata": {},
            "source": [
                "# SIR model: inverse problem\n",
                "## A PINN approach\n",
                "\n",
                "In this notebook, we will solve the inverse problem of the SIR model using a Physics-Informed Neural Network (PINN). The goal is to estimate the infection rate $\\beta$ from the observed data of the infected population. To do this, we will train a PINN model, where we compute the residuals of the differential equation system with initial conditions and the data loss simultaneously.\n",
                "\n",
                "The SIR model is governed by the following set of ordinary differential equations (ODEs):\n",
                "\n",
                "$$\n",
                "\\begin{cases}\n",
                "\\frac{dS}{dt} &= -\\frac{\\beta}{N} I S, \\\\\n",
                "\\frac{dI}{dt} &= \\frac{\\beta}{N} I S - \\delta I, \\\\\n",
                "\\frac{dR}{dt} &= \\delta I,\n",
                "\\end{cases}\n",
                "$$\n",
                "\n",
                "where $t \\in [0, 90]$ and with the initial conditions $S(0) = N - 1$, $I(0) = 1$, and $R(0) = 0$."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2b13f713fe59120",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "4e31b153caf564bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pytorch_lightning as pl\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from dataclasses import dataclass, field\n",
                "from typing import List, Tuple, Dict, Optional, Union\n",
                "from scipy.integrate import odeint\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Create directories\n",
                "figures_dir = \"figures\"\n",
                "data_dir = \"data\"\n",
                "os.makedirs(figures_dir, exist_ok=True)\n",
                "os.makedirs(data_dir, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "638b5816",
            "metadata": {},
            "source": [
                "## Module's components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "4a5013ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Square(nn.Module):\n",
                "    \"\"\"A module that squares its input element-wise.\"\"\"\n",
                "    @staticmethod\n",
                "    def forward(x):\n",
                "        return torch.square(x)\n",
                "\n",
                "\n",
                "def create_mlp(layers_dims, activation, output_activation):\n",
                "    \"\"\"Create a multi-layer perceptron with specified architecture.\"\"\"\n",
                "    layers = []\n",
                "    for i in range(len(layers_dims) - 1):\n",
                "        layers.append(nn.Linear(layers_dims[i], layers_dims[i + 1]))\n",
                "        if i < len(layers_dims) - 2:\n",
                "            layers.append(activation)\n",
                "    layers.append(output_activation)\n",
                "\n",
                "    net = nn.Sequential(*layers)\n",
                "\n",
                "    for layer in net:\n",
                "        if isinstance(layer, nn.Linear):\n",
                "            nn.init.xavier_normal_(layer.weight)\n",
                "            nn.init.zeros_(layer.bias)\n",
                "\n",
                "    return net"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2ffcfb17",
            "metadata": {},
            "source": [
                "## Module's configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "33bd675d",
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class SIRConfig:\n",
                "    \"\"\"Configuration for SIR PINN model and training.\"\"\"\n",
                "    # Model parameters\n",
                "    N: float = 1.0\n",
                "    delta: float = 0.2\n",
                "    initial_beta: float = 0.5\n",
                "    \n",
                "    # Neural network architecture\n",
                "    hidden_layers: List[int] = field(default_factory=lambda: 4 * [50])\n",
                "    activation: str = 'tanh'\n",
                "    output_activation: str = 'square'\n",
                "    \n",
                "    # Initial conditions\n",
                "    initial_conditions: List[float] = field(default_factory=lambda: [1.0 - 1e-6, 1e-6, 0.0])\n",
                "    \n",
                "    # Training parameters\n",
                "    learning_rate: float = 1e-3\n",
                "    batch_size: int = 100\n",
                "    max_epochs: int = 5000\n",
                "    \n",
                "    # Scheduler parameters\n",
                "    scheduler_factor: float = 0.5\n",
                "    scheduler_patience: int = 100\n",
                "    scheduler_threshold: float = 0.01\n",
                "    scheduler_min_lr: float = 1e-6\n",
                "    \n",
                "    # Early stopping\n",
                "    early_stopping_patience: int = 200\n",
                "    \n",
                "    # Loss weights\n",
                "    pde_weight: float = 0.1\n",
                "    ic_weight: float = 0.45\n",
                "    data_weight: float = 0.45\n",
                "    \n",
                "    # Dataset parameters\n",
                "    time_domain: Tuple[float, float] = (0.0, 90.0)\n",
                "    collocation_points: int = 6000\n",
                "    \n",
                "    # Logging\n",
                "    log_every_n_steps: int = 100"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7120ac1ce626e51c",
            "metadata": {},
            "source": [
                "## Dataset creation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "7602c2b769e44162",
            "metadata": {},
            "outputs": [],
            "source": [
                "class SIRDataset(Dataset):\n",
                "    \"\"\"Dataset for SIR PINN training.\"\"\"\n",
                "    def __init__(\n",
                "        self, \n",
                "        t_obs: np.ndarray, \n",
                "        i_obs: np.ndarray, \n",
                "        time_domain: Tuple[float, float] = (0.0, 90.0), \n",
                "        n_collocation: int = 6000\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Initialize dataset with observation points and random collocation points.\n",
                "        \n",
                "        Args:\n",
                "            t_obs: Observation time points\n",
                "            i_obs: Observed infected population at each time point\n",
                "            time_domain: (t_min, t_max) time range\n",
                "            n_collocation: Number of random collocation points to generate\n",
                "        \"\"\"\n",
                "        self.t_obs = torch.tensor(t_obs, dtype=torch.float32).reshape(-1, 1)\n",
                "        self.i_obs = torch.tensor(i_obs, dtype=torch.float32).reshape(-1, 1)\n",
                "        \n",
                "        # Generate random collocation points with log-uniform distribution\n",
                "        t_min, t_max = time_domain\n",
                "        rand_t = np.expm1(np.random.uniform(np.log1p(t_min), np.log1p(t_max), n_collocation))\n",
                "        self.t_collocation = torch.tensor(rand_t, dtype=torch.float32).reshape(-1, 1)\n",
                "        \n",
                "        self.t_combined = torch.cat([self.t_obs, self.t_collocation], dim=0)\n",
                "        \n",
                "        # Create a mask to identify observation points\n",
                "        self.is_obs = torch.zeros(len(self.t_combined), dtype=torch.bool)\n",
                "        self.is_obs[:len(self.t_obs)] = True\n",
                "        \n",
                "        # Create target values for all points (observations have actual values, collocation are zeros)\n",
                "        self.i_targets = torch.zeros(len(self.t_combined), 1, dtype=torch.float32)\n",
                "        self.i_targets[:len(self.t_obs)] = self.i_obs\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.t_combined)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        return {\n",
                "            't': self.t_combined[idx],\n",
                "            'is_obs': self.is_obs[idx],\n",
                "            'i_target': self.i_targets[idx]\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb36a9738f8dd15c",
            "metadata": {},
            "source": [
                "## Module definition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "998d6e196fb0334a",
            "metadata": {},
            "outputs": [],
            "source": [
                "class SIRPINN(pl.LightningModule):\n",
                "    \"\"\"Physics-Informed Neural Network for SIR model parameter identification.\"\"\"\n",
                "    \n",
                "    def __init__(self, config: SIRConfig):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        self.config = config\n",
                "        \n",
                "        # Set activation functions\n",
                "        activation_map = {\n",
                "            'tanh': nn.Tanh(),\n",
                "            'relu': nn.ReLU(),\n",
                "            'leaky_relu': nn.LeakyReLU(),\n",
                "            'sigmoid': nn.Sigmoid(),\n",
                "            'selu': nn.SELU()\n",
                "        }\n",
                "        \n",
                "        output_activation_map = {\n",
                "            'square': Square(),\n",
                "            'softplus': nn.Softplus(),\n",
                "            'identity': nn.Identity()\n",
                "        }\n",
                "        \n",
                "        activation = activation_map.get(config.activation, nn.Tanh())\n",
                "        output_activation = output_activation_map.get(config.output_activation, Square())\n",
                "        \n",
                "        # Define network architecture\n",
                "        layers_dims = [1] + config.hidden_layers + [1]\n",
                "        \n",
                "        # Create networks for S and I compartments\n",
                "        self.net_S = create_mlp(layers_dims, activation, output_activation)\n",
                "        self.net_I = create_mlp(layers_dims, activation, output_activation)\n",
                "        \n",
                "        # Initialize learnable parameter beta\n",
                "        self.beta = nn.Parameter(torch.tensor(config.initial_beta, dtype=torch.float32))\n",
                "        \n",
                "        # Fixed parameters\n",
                "        self.N = config.N\n",
                "        self.delta = config.delta\n",
                "        \n",
                "        # Loss function\n",
                "        self.loss_fn = nn.MSELoss()\n",
                "        \n",
                "        # For tracking metrics\n",
                "        self.beta_history = []\n",
                "        \n",
                "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass to compute S, I, R values at time t.\n",
                "        \n",
                "        Args:\n",
                "            t: Time points tensor of shape [batch_size, 1]\n",
                "            \n",
                "        Returns:\n",
                "            Tensor of shape [batch_size, 3] with [S, I, R] values\n",
                "        \"\"\"\n",
                "        S = self.net_S(t)\n",
                "        I = self.net_I(t)\n",
                "        R = self.N - S - I\n",
                "        \n",
                "        return torch.cat([S, I, R], dim=1)\n",
                "    \n",
                "    def compute_ode_residuals(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        \"\"\"\n",
                "        Compute residuals of the SIR ODEs using automatic differentiation.\n",
                "        \n",
                "        Args:\n",
                "            t: Time points tensor of shape [batch_size, 1]\n",
                "            \n",
                "        Returns:\n",
                "            Tuple of residual tensors (res_S, res_I)\n",
                "        \"\"\"\n",
                "        t_tensor = t.clone().detach().requires_grad_(False)\n",
                "    \n",
                "        S = self.net_S(t_tensor)\n",
                "        I = self.net_I(t_tensor)\n",
                "        \n",
                "        dS_dt = torch.autograd.grad(\n",
                "            S, t_tensor, grad_outputs=torch.ones_like(S), create_graph=True\n",
                "        )[0]\n",
                "        \n",
                "        dI_dt = torch.autograd.grad(\n",
                "            I, t_tensor, grad_outputs=torch.ones_like(I), create_graph=True\n",
                "        )[0]\n",
                "\n",
                "        \n",
                "        res_S = dS_dt + self.beta * S * I\n",
                "        res_I = dI_dt - self.beta * S * I + self.delta * I\n",
                "        \n",
                "        return res_S, res_I\n",
                "    \n",
                "    def pde_loss(self, t: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Compute PDE residual loss.\"\"\"\n",
                "        res_S, res_I = self.compute_ode_residuals(t)\n",
                "        loss_S = self.loss_fn(res_S, torch.zeros_like(res_S))\n",
                "        loss_I = self.loss_fn(res_I, torch.zeros_like(res_I))\n",
                "        return loss_S + loss_I\n",
                "    \n",
                "    def ic_loss(self) -> torch.Tensor:\n",
                "        \"\"\"Compute initial condition loss.\"\"\"\n",
                "        t0 = torch.zeros(1, 1, device=self.device, dtype=torch.float32)\n",
                "        ic_pred = self(t0)\n",
                "        ic_true = torch.tensor(\n",
                "            self.config.initial_conditions, \n",
                "            device=self.device,\n",
                "            dtype=torch.float32\n",
                "        ).reshape(1, 3)\n",
                "        return self.loss_fn(ic_pred, ic_true)\n",
                "    \n",
                "    def data_loss(self, t_obs: torch.Tensor, i_obs: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Compute data fitting loss.\"\"\"\n",
                "        if t_obs.shape[0] == 0:  # No observations in batch\n",
                "            return torch.tensor(0.0, device=self.device)\n",
                "        \n",
                "        i_pred = self(t_obs)[:, 1:2]  # Extract I component\n",
                "        return self.loss_fn(i_pred, i_obs)\n",
                "    \n",
                "    def training_step(self, batch, batch_idx):\n",
                "        # Extract batch data\n",
                "        t = batch['t']\n",
                "        is_obs = batch['is_obs']\n",
                "        i_target = batch['i_target']\n",
                "        \n",
                "        # Split into observation and collocation points\n",
                "        t_obs = t[is_obs] if is_obs.any() else torch.zeros((0, 1), device=self.device)\n",
                "        i_obs = i_target[is_obs] if is_obs.any() else torch.zeros((0, 1), device=self.device)\n",
                "        \n",
                "        # Compute losses\n",
                "        pde_loss_val = self.pde_loss(t)\n",
                "        ic_loss_val = self.ic_loss()\n",
                "        data_loss_val = self.data_loss(t_obs, i_obs)\n",
                "        \n",
                "        # Weighted sum of losses\n",
                "        total_loss = (\n",
                "            self.config.pde_weight * pde_loss_val +\n",
                "            self.config.ic_weight * ic_loss_val +\n",
                "            self.config.data_weight * data_loss_val\n",
                "        )\n",
                "        \n",
                "        # Log metrics\n",
                "        self.log('train/pde_loss', pde_loss_val, prog_bar=True)\n",
                "        self.log('train/ic_loss', ic_loss_val, prog_bar=True)\n",
                "        self.log('train/data_loss', data_loss_val, prog_bar=True)\n",
                "        self.log('train/total_loss', total_loss, prog_bar=True)\n",
                "        self.log('train/beta', self.beta.item(), prog_bar=True)\n",
                "        \n",
                "        # Track beta history\n",
                "        self.beta_history.append(self.beta.item())\n",
                "        \n",
                "        return total_loss\n",
                "    \n",
                "    def validation_step(self, batch, batch_idx):\n",
                "        # Same computation as training but for validation set\n",
                "        t = batch['t']\n",
                "        is_obs = batch['is_obs']\n",
                "        i_target = batch['i_target']\n",
                "        \n",
                "        t_obs = t[is_obs] if is_obs.any() else torch.zeros((0, 1), device=self.device)\n",
                "        i_obs = i_target[is_obs] if is_obs.any() else torch.zeros((0, 1), device=self.device)\n",
                "        \n",
                "        pde_loss_val = self.pde_loss(t)\n",
                "        ic_loss_val = self.ic_loss()\n",
                "        data_loss_val = self.data_loss(t_obs, i_obs)\n",
                "        \n",
                "        total_loss = (\n",
                "            self.config.pde_weight * pde_loss_val +\n",
                "            self.config.ic_weight * ic_loss_val +\n",
                "            self.config.data_weight * data_loss_val\n",
                "        )\n",
                "        \n",
                "        self.log('val/pde_loss', pde_loss_val)\n",
                "        self.log('val/ic_loss', ic_loss_val)\n",
                "        self.log('val/data_loss', data_loss_val)\n",
                "        self.log('val/total_loss', total_loss)\n",
                "        \n",
                "        return total_loss\n",
                "    \n",
                "    def predict_sir(self, t):\n",
                "        \"\"\"Predict SIR values at specified time points.\"\"\"\n",
                "        with torch.no_grad():\n",
                "            t_tensor = torch.tensor(t, dtype=torch.float32).reshape(-1, 1)\n",
                "            return self(t_tensor).cpu().numpy()\n",
                "    \n",
                "    def configure_optimizers(self):\n",
                "        optimizer = torch.optim.Adam(\n",
                "            self.parameters(), \n",
                "            lr=self.config.learning_rate\n",
                "        )\n",
                "        \n",
                "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
                "            optimizer,\n",
                "            mode='min',\n",
                "            factor=self.config.scheduler_factor,\n",
                "            patience=self.config.scheduler_patience,\n",
                "            threshold=self.config.scheduler_threshold,\n",
                "            min_lr=self.config.scheduler_min_lr\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            'optimizer': optimizer,\n",
                "            'lr_scheduler': {\n",
                "                'scheduler': scheduler,\n",
                "                'monitor': 'val/total_loss',\n",
                "                'interval': 'epoch',\n",
                "                'frequency': 1\n",
                "            }\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1e0b9eaa",
            "metadata": {},
            "source": [
                "## Training definition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "45bc6a5a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_sir_pinn(t_obs, i_obs, override_config=None):\n",
                "    \"\"\"\n",
                "    Train a SIR PINN model using the provided observations.\n",
                "    \n",
                "    Args:\n",
                "        t_obs: Observation time points\n",
                "        i_obs: Observed infected proportions\n",
                "        override_config: Configuration object (optional)\n",
                "        \n",
                "    Returns:\n",
                "        Trained PINN model\n",
                "    \"\"\"\n",
                "    if override_config is None:\n",
                "        config = SIRConfig()\n",
                "    else:\n",
                "        config = override_config\n",
                "    \n",
                "    # Create dataset and dataloaders\n",
                "    dataset = SIRDataset(\n",
                "        t_obs=t_obs,\n",
                "        i_obs=i_obs,\n",
                "        time_domain=config.time_domain,\n",
                "        n_collocation=config.collocation_points\n",
                "    )\n",
                "    \n",
                "    # Split into train/validation\n",
                "    train_size = int(0.8 * len(dataset))\n",
                "    val_size = len(dataset) - train_size\n",
                "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
                "    \n",
                "    train_loader = DataLoader(\n",
                "        train_dataset,\n",
                "        batch_size=config.batch_size,\n",
                "        shuffle=True,\n",
                "        num_workers=0\n",
                "    )\n",
                "    \n",
                "    val_loader = DataLoader(\n",
                "        val_dataset,\n",
                "        batch_size=config.batch_size,\n",
                "        shuffle=False,\n",
                "        num_workers=0\n",
                "    )\n",
                "    \n",
                "    # Create model\n",
                "    model = SIRPINN(config)\n",
                "    \n",
                "    # Setup callbacks\n",
                "    callbacks = [\n",
                "        pl.callbacks.EarlyStopping(\n",
                "            monitor='val/total_loss',\n",
                "            patience=config.early_stopping_patience,\n",
                "            mode='min'\n",
                "        ),\n",
                "        pl.callbacks.ModelCheckpoint(\n",
                "            monitor='val/total_loss',\n",
                "            filename='sir-pinn-{epoch:02d}-{val/total_loss:.4e}',\n",
                "            save_top_k=1,\n",
                "            mode='min'\n",
                "        ),\n",
                "        pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
                "    ]\n",
                "    \n",
                "    # Create trainer\n",
                "    trainer = pl.Trainer(\n",
                "        max_epochs=config.max_epochs,\n",
                "        callbacks=callbacks,\n",
                "        log_every_n_steps=config.log_every_n_steps,\n",
                "        logger=pl.loggers.TensorBoardLogger('lightning_logs', name='sir_pinn')\n",
                "    )\n",
                "    \n",
                "    # Train model\n",
                "    trainer.fit(model, train_loader, val_loader)\n",
                "    \n",
                "    # Load best model\n",
                "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
                "    if best_model_path:\n",
                "        model = SIRPINN.load_from_checkpoint(best_model_path)\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fe623e70",
            "metadata": {},
            "source": [
                "## Execution (with synthetic data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "58d0265a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "GPU available: True (mps), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "\n",
                        "  | Name         | Type       | Params | Mode \n",
                        "----------------------------------------------------\n",
                        "0 | net_S        | Sequential | 7.8 K  | train\n",
                        "1 | net_I        | Sequential | 7.8 K  | train\n",
                        "2 | loss_fn      | MSELoss    | 0      | train\n",
                        "  | other params | n/a        | 1      | n/a  \n",
                        "----------------------------------------------------\n",
                        "15.6 K    Trainable params\n",
                        "0         Non-trainable params\n",
                        "15.6 K    Total params\n",
                        "0.062     Total estimated model params size (MB)\n",
                        "15        Modules in train mode\n",
                        "0         Modules in eval mode\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4fc9c357b46b41d19b6674fd4ec3d055",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/giacomo/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[14], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m r_true \u001b[38;5;241m=\u001b[39m solution[:, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     31\u001b[0m i_obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpoisson(i_true)\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sir_pinn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_obs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[13], line 73\u001b[0m, in \u001b[0;36mtrain_sir_pinn\u001b[0;34m(t_obs, i_obs, override_config)\u001b[0m\n\u001b[1;32m     65\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     66\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_epochs,\n\u001b[1;32m     67\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     68\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlog_every_n_steps,\n\u001b[1;32m     69\u001b[0m     logger\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mloggers\u001b[38;5;241m.\u001b[39mTensorBoardLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msir_pinn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[1;32m     76\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[12], line 161\u001b[0m, in \u001b[0;36mSIRPINN.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    158\u001b[0m t_obs \u001b[38;5;241m=\u001b[39m t[is_obs] \u001b[38;5;28;01mif\u001b[39;00m is_obs\u001b[38;5;241m.\u001b[39many() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    159\u001b[0m i_obs \u001b[38;5;241m=\u001b[39m i_target[is_obs] \u001b[38;5;28;01mif\u001b[39;00m is_obs\u001b[38;5;241m.\u001b[39many() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 161\u001b[0m pde_loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpde_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m ic_loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mic_loss()\n\u001b[1;32m    163\u001b[0m data_loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loss(t_obs, i_obs)\n",
                        "Cell \u001b[0;32mIn[12], line 94\u001b[0m, in \u001b[0;36mSIRPINN.pde_loss\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpde_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, t: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute PDE residual loss.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     res_S, res_I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_ode_residuals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     loss_S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(res_S, torch\u001b[38;5;241m.\u001b[39mzeros_like(res_S))\n\u001b[1;32m     96\u001b[0m     loss_I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(res_I, torch\u001b[38;5;241m.\u001b[39mzeros_like(res_I))\n",
                        "Cell \u001b[0;32mIn[12], line 78\u001b[0m, in \u001b[0;36mSIRPINN.compute_ode_residuals\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     75\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_S(t_tensor)\n\u001b[1;32m     76\u001b[0m I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_I(t_tensor)\n\u001b[0;32m---> 78\u001b[0m dS_dt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     82\u001b[0m dI_dt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     83\u001b[0m     I, t_tensor, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(I), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     84\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     87\u001b[0m res_S \u001b[38;5;241m=\u001b[39m dS_dt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m*\u001b[39m S \u001b[38;5;241m*\u001b[39m I\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
                        "File \u001b[0;32m~/.conda/envs/pinn/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
                    ]
                }
            ],
            "source": [
                "# True SIR parameters for data generation\n",
                "N = 56e6  # Total population (for scaling)\n",
                "delta = 1 / 5  # Recovery rate (per day)\n",
                "r0 = 3.0  # Basic reproduction number\n",
                "beta_true = delta * r0  # Infection rate (per day)\n",
                "\n",
                "\n",
                "def sir(x, _, d, b):\n",
                "  s, i, r = x\n",
                "  l = b * i / N\n",
                "  ds_dt = -l * s\n",
                "  di_dt = l * s - d * i\n",
                "  dr_dt = d * i\n",
                "  return np.array([ds_dt, di_dt, dr_dt])\n",
                "\n",
                "\n",
                "# Initial conditions (actual population counts)\n",
                "S0 = N - 1\n",
                "I0 = 1\n",
                "R0 = 0\n",
                "\n",
                "# Time domain for simulation\n",
                "t_start, t_end = 0, 90  # days\n",
                "t_obs = np.linspace(t_start, t_end, t_end - t_start + 1)\n",
                "\n",
                "solution = odeint(sir, [S0, I0, R0], t_obs, args=(delta, beta_true))\n",
                "s_true = solution[:, 0]\n",
                "i_true = solution[:, 1]\n",
                "r_true = solution[:, 2]\n",
                "\n",
                "i_obs = np.random.poisson(i_true)\n",
                "\n",
                "\n",
                "model = train_sir_pinn(t_obs, i_obs)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f2d0a782",
            "metadata": {},
            "source": [
                "## Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1af5d8b",
            "metadata": {},
            "outputs": [],
            "source": [
                "t_eval = np.linspace(t_start, t_end, 1000)\n",
                "pred_sir = model.predict_sir(t_eval)\n",
                "pred_beta = model.beta.item()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d5fde243",
            "metadata": {},
            "source": [
                "Model predition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2e75ea2",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "\n",
                "sns.lineplot(x=t_obs, y=i_obs, label='True', color='black', linewidth=2)\n",
                "sns.lineplot(x=t_eval, y=pred_sir[:, 1], label='PINN', color='red', linewidth=2)\n",
                "\n",
                "plt.xlabel('Time (days)')\n",
                "plt.ylabel('Infected population')\n",
                "plt.title(f'SIR Model with PINN (Estimated $\\beta$ = {pred_beta:.4f})')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5f4ea337",
            "metadata": {},
            "source": [
                "Beta evolution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0af0c9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(model.beta_history, label='PINN')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Beta')\n",
                "plt.title('Beta Evolution')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pinn",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.21"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
